{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the statistics of jokes and none-jokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data storage\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Language processing\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "\n",
    "#Other\n",
    "import random\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General stats about the count of words and sentences\n",
    "def print_tokenization_stats(df):\n",
    "    print(f'Text instances: {df.shape[0]:,}\\n')\n",
    "    \n",
    "    word_lengths_series = df[\"word_tokenize\"].apply(len)\n",
    "    print(f'Words total: {word_lengths_series.sum():,}')\n",
    "    print(f'Words mean: {word_lengths_series.mean():.2f}')\n",
    "    print(f'Words std: {word_lengths_series.std():.2f}')\n",
    "    print()\n",
    "      \n",
    "    sentence_lengths_series = df[\"sent_tokenize\"].apply(len)\n",
    "    print(f'Sentences total: {sentence_lengths_series.sum():,}')\n",
    "    print(f'Sentences mean: {sentence_lengths_series.mean():.2f}')\n",
    "    print(f'Sentences std: {sentence_lengths_series.std():.2f}')\n",
    "    print()\n",
    "    \n",
    "\n",
    "# Creates and returns FreqDist's for use in later cells\n",
    "def word_analysis(df):\n",
    "    freq_dist = FreqDist()\n",
    "    unique_freq_dist = FreqDist()\n",
    "    \n",
    "    for tokens in df[\"word_tokenize\"]:\n",
    "\n",
    "        freq_dist.update(tokens)\n",
    "        unique_freq_dist.update(set(tokens))\n",
    "        \n",
    "    print(\"General frequencies:\")\n",
    "    freq_dist.pprint(20)\n",
    "    print(\"\\nUnique frequencies(each token counted only once per text instance):\")\n",
    "    unique_freq_dist.pprint(20)\n",
    "    \n",
    "    return freq_dist, unique_freq_dist\n",
    "\n",
    "\n",
    "# Combines the functions above\n",
    "def analyze_file(filename):\n",
    "    df = pd.read_json(filename)\n",
    "    \n",
    "    print_tokenization_stats(df)\n",
    "    freq_dist, unique_freq_dist = word_analysis(df)\n",
    "    \n",
    "    return freq_dist, unique_freq_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data from tokenized files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jokes\n",
      "Text instances: 962,887\n",
      "\n",
      "Words total: 50,929,587\n",
      "Words mean: 52.89\n",
      "Words std: 114.07\n",
      "\n",
      "Sentences total: 3,584,434\n",
      "Sentences mean: 3.72\n",
      "Sentences std: 7.52\n",
      "\n",
      "General frequencies:\n",
      "FreqDist({'.': 2188302, ',': 1965801, 'the': 1917239, 'a': 1289612, 'and': 1050540, 'to': 994668, 'I': 938192, \"''\": 753988, '?': 736444, '``': 686423, 'you': 607805, 'of': 516864, 'in': 483904, 'The': 442109, 'he': 400837, 'it': 398768, '!': 393848, 'is': 387391, \"'s\": 354145, 'was': 342901, ...})\n",
      "\n",
      "Unique frequencies(each token counted only once per text instance):\n",
      "FreqDist({'.': 641614, 'a': 538387, '?': 518458, 'the': 510573, 'I': 382737, ',': 382527, 'to': 381310, 'and': 336873, 'you': 320106, 'in': 267152, 'of': 264362, 'it': 229037, 'What': 226421, 'is': 225860, \"'s\": 219446, 'do': 212499, \"''\": 196765, '``': 191940, 'The': 183609, 'was': 181501, ...})\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "news\n",
      "Text instances: 300,000\n",
      "\n",
      "Words total: 42,018,000\n",
      "Words mean: 140.06\n",
      "Words std: 92.44\n",
      "\n",
      "Sentences total: 1,452,614\n",
      "Sentences mean: 4.84\n",
      "Sentences std: 2.79\n",
      "\n",
      "General frequencies:\n",
      "FreqDist({',': 1964525, 'the': 1859372, '.': 1375651, 'to': 960997, 'of': 920739, 'a': 853142, 'and': 836226, 'in': 832144, '’': 510022, 's': 401374, 'on': 376192, 'for': 340482, '“': 325767, 'that': 321536, '”': 315801, 'The': 312748, 'by': 309229, 'said': 308192, 'is': 251568, 'with': 237235, ...})\n",
      "\n",
      "Unique frequencies(each token counted only once per text instance):\n",
      "FreqDist({'.': 293478, ',': 279207, 'the': 276995, 'to': 248300, 'of': 246745, 'in': 246065, 'a': 245141, 'and': 242720, '’': 199828, 'on': 184370, 's': 182071, 'The': 177704, 'for': 175383, 'by': 162112, 'said': 160786, 'that': 157889, '“': 151205, '”': 149301, 'with': 140934, 'is': 138505, ...})\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "google_qa\n",
      "Text instances: 307,373\n",
      "\n",
      "Words total: 46,962,326\n",
      "Words mean: 152.79\n",
      "Words std: 420.23\n",
      "\n",
      "Sentences total: 1,140,794\n",
      "Sentences mean: 3.71\n",
      "Sentences std: 3.38\n",
      "\n",
      "General frequencies:\n",
      "FreqDist({',': 2021471, 'the': 1920694, '.': 1076750, 'of': 978238, '(': 835831, ')': 835716, 'and': 762475, 'in': 702994, '-': 644382, '``': 557707, 'a': 557560, 'to': 547901, '--': 442642, 'is': 351374, '/': 246390, 'as': 244014, 'by': 240262, 'on': 231638, ':': 224038, 'was': 222751, ...})\n",
      "\n",
      "Unique frequencies(each token counted only once per text instance):\n",
      "FreqDist({'the': 285596, '.': 262666, ',': 260041, 'of': 248855, 'in': 236245, 'and': 229044, 'a': 197744, 'to': 186541, 'is': 164249, '(': 149907, ')': 149874, 'by': 129350, '-': 125616, 'on': 124665, 'as': 121293, 'for': 120254, 'was': 113917, \"'s\": 107305, 'with': 103561, 'The': 101826, ...})\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "files = [\"jokes\", \"news\", \"google_qa\"]\n",
    "freq_dists = []\n",
    "unique_freq_dists = []\n",
    "\n",
    "for f in files:\n",
    "    print(f)\n",
    "    freq_dist, unique_freq_dist = analyze_file(f'data/stats_files/tokenized_{f}.json.xz')\n",
    "    freq_dists.append(freq_dist)\n",
    "    unique_freq_dists.append(unique_freq_dist)\n",
    "    print(\"\\n--------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differences in most frequent words\n",
    "Unique nltk.word_tokenize() tokens among the {nr} most common tokens in each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens that appear among the 500 most frequent words in dataset, but not among the top 500 of other 2 datasets.\n",
      "\n",
      "In joke but not in others: \n",
      "{'every', 'fuck', 'Two', 'thinks', 'favorite', 'Do', 'jokes', 'women', 'looked', 'thought', 'says', 'talk', 'sitting', 'turned', 'shit', 'na', 'pretty', 'room', 'anything', 'Then', '[', 'believe', 'enough', 'everything', 'takes', 'bar', 'lot', 'married', 'Yes', 'beer', 'hear', 'gon', '....', 'decided', 'legs', 'tell', 'walk', 'moment', 'something', 'heard', 'friends', 'beautiful', 'done', \"'ll\", 'behind', 'wrong', 'mom', 'ass', 'Now', 'light', 'died', '..', 'please', 'Just', 'tells', 'never', 'starts', 'hell', 'trying', 'buy', 'car', 'nothing', 'God', 'An', \"'m\", 'went', 'guy', 'driving', 'ever', 'saw', 'One', 'am', 'dad', 'everyone', 'keep', 'gets', 'turn', 'walks', 'happened', 'difference', 'bit', 'guess', 'asks', 'eyes', \"'ve\", 'problem', 'morning', 'fucking', 'minutes', \"'re\", 'face', 'lady', 'woman', 'So', 'cross', 'sorry', 'funny', 'amp', 'someone', 'better', 'sees', 'die', 'dog', 'food', 'started', 'Finally', 'word', 'hand', 'girlfriend', 'stop', 'Can', 'your', 'find', 'bed', 'His', 'doing', 'actually', 'To', 'front', 'Because', 'Well', 'ca', 'goes', 'once', 'dick', 'All', 'hands', 'responds', 'drink', 'No', 'Have', 'boy', 'looks', 'gave', 'girl', 'why', 'sex', 'walking', 'talking', 'teacher', '#', 'penis', 'kid', 'today', ']', 'makes', 'Did', 'Hey', 'ask', 'doctor', 'couple', 'got', 'myself', 'tried', 'job', 'kids', 'things', 'walked', 'baby', 'road', 'My', 'always', 'replies', 'Man', 'table', 'else', 'Three', 'bad', 'outside', '*', 'try', 'Why', 'eat', 'sure', 'hard', 'Your', 'nice', 'inside', 'store', 'dead', 'wanted', 'door', 'Dad', 'son', 'joke', 'feel', 'knew', 'turns', 'Is', 'wife', 'question', 'bartender', 'whole', 'Not', 'mean', 'Me', 'finally', 'guys', 'Oh', 'replied', 'gives', 'leave', 'gay', 'let', 'wants', 'You', \"'d\", 'looking', 'look', 'decides', 'himself', 'friend', 'getting', 'husband', 'course', 'Are', 'really', 'kind', 'thing', 'hours'}\n",
      "\n",
      "In news but not in others: \n",
      "{'chief', 'months', 'potential', 'agreement', 'China', 'debt', 'Department', 'National', 'clear', 'director', 'prices', 'billion', 'leader', 'Russia', 'case', 'conference', 'news', 'banks', 'support', 'shares', 'rise', 'local', 'campaign', 'meeting', 'Writing', 'previous', 'sales', 'possible', 'Washington', 'showed', 'House', 'South', 'bank', 'growth', 'executive', 'according', 'share', 'security', 'Sunday', 'crisis', 'December', 'vote', 'lower', 'past', 'strong', 'added', 'Wednesday', 'markets', 'low', 'Mr.', 'Reporting', 'stock', 'risk', 'expected', 'plans', 'Minister', 'June', 'economic', 'percent', 'increase', 'New', 'killed', 'Britain', 'services', 'U.S.', 'York', 'future', 'continue', 'rates', 'biggest', 'Chief', 'rate', 'Group', 'pressure', 'though', 'per', 'ago', 'Europe', 'data', 'America', 'month', 'taking', 'Union', 'price', 'business', 'State', 'investors', 'January', 'media', 'John', 'ahead', 'comment', 'adding', 'Thursday', 'market', 'talks', 'senior', 'May', 'administration', 'demand', 'trade', 'capital', 'declined', 'United', 'spokesman', 'Donald', 'points', 'Friday', 'States', 'Bank', 'industry', 'military', 'economy', 'latest', 'plan', '2019', 'ended', 'index', 'editing', 'EU', 'President', 'health', 'working', 'trading', 'far', 'annual', 'financial', 'authorities', 'deal', 'foreign', 'less', 'further', 'taken', 'Monday', 'rose', 'weeks', 'private', 'policy', 'decision', 'April', 'political', '—', 'win', 'near', 'North', 'British', 'likely', 'March', 'sector', 'July', 'recent', 'global', 'officials', 'nearly', 'close', 'American', 'Chinese', 'investment', 'least', 'interest', 'interview', 'higher', 'yet', 'pay', 'election', 'firm', 'agreed', 'whether', 'profit', 'reported', 'oil', 'report', 'reporters', 'half', 'earlier', 'quarter', 'companies', 'statement', 'European', 'immediately', 'Saturday', 'minister', 'Editing', 'agency', 'move', 'announced', 'London', 'cut', 'Tuesday', 'whose'}\n",
      "\n",
      "In qa but not in others: \n",
      "{'language', 'september', 'union', 'single', '2007', 'star', 'plays', 'player', '1994', '2012', '27', 'became', 'actor', 'michael', 'date', 'canada', 'region', 'november', 'free', 'san', '1991', 'considered', '18', 'include', 'named', 'often', 'george', 'act', '40', 'india', 'television', 'october', '100', 'australia', '2005', '23', 'various', 'american', 'based', 'west', 'version', 'america', '2011', 'main', 'control', 'age', 'however', 'created', 'book', 'english', 'tv', 'paul', 'january', 'western', 'episodes', 'different', 'example', '13', 'episode', 'ii', 'england', 'de', 'air', 'title', '24', 'history', '0', '2000', 'french', 'character', '16', 'british', 'united', 'established', '17', 'recorded', 'germany', '1986', 'children', '2010', 'term', '1992', 'republic', '21', 'usually', '2006', 'result', 'john', '6', 'general', 'washington', 'east', 'river', 'original', 'popular', 'producer', 'david', '2001', 'included', 'story', 'using', '2013', '7', 'list', 'kingdom', 'born', 'league', 'indian', 'Which', '29', 'along', '1990', 'n', 'north', '50', 'located', 'role', '19', 'form', 'york', '1999', '2003', '8', 'played', 'july', 'rock', '--', 'august', '31', '35', 'king', 'park', '1993', 'u.s.', 'island', 'human', 'b', 'present', '26', 'century', '14', '25', 'south', 'location', '2008', 'written', 'member', 'order', 'length', '32', 'music', '28', '9', 'singer', '/', 'april', 'although', '1995', 'france', 'band', 'given', 'movie', 'cast', '2004', 'song', 'However', 'states', 'film', 'robert', 'originally', 'directed', 'population', 'los', 'video', 'wrote', 'land', '1998', '2009', 'death', 'games', 'modern', '22', '1997', 'february', 'california', 'release', 'body', 'album', '\\\\', 'type', 'within', 'area', '2002', 'june', 'produced', 'james', 'red', 'december', 'march', 'released', 'uk'}\n"
     ]
    }
   ],
   "source": [
    "# most_common_count - Number of most common words taken from each dataset\n",
    "def token_differences(freq_dists, unique_freq_dists, most_common_count = 100):\n",
    "    most_common_uniques = []\n",
    "    \n",
    "    for dist in unique_freq_dists:\n",
    "        uniques = set(x[0] for x in dist.most_common(most_common_count))\n",
    "        most_common_uniques.append(uniques)\n",
    "        \n",
    "        \n",
    "    print(f'In joke but not in others: \\n{most_common_uniques[0] - most_common_uniques[1] - most_common_uniques[2]}')\n",
    "    print()\n",
    "    print(f'In news but not in others: \\n{most_common_uniques[1] - most_common_uniques[0] - most_common_uniques[2]}')\n",
    "    print()\n",
    "    print(f'In qa but not in others: \\n{most_common_uniques[2] - most_common_uniques[0] - most_common_uniques[1]}')\n",
    "\n",
    "\n",
    "# This number can be changed\n",
    "most_common_count = 500\n",
    "print(f'Tokens that appear among the {most_common_count} most frequent words in dataset, but not among the top {most_common_count} of other 2 datasets.\\n')\n",
    "token_differences(freq_dists, unique_freq_dists, most_common_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset specific statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Prints first 20 news articles\n",
    "def news_lookat():\n",
    "    news = pd.read_json(f'data/stats_files/tokenized_news.json.xz')\n",
    "    i = 0\n",
    "    for text in news.text:\n",
    "        print(text)\n",
    "        print(\"\\n----------------------------------------------------------\\n\")\n",
    "        if i > 20:\n",
    "            break\n",
    "        i+=1\n",
    "\n",
    "#news_lookat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
