{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "import gensim.downloader as api\n",
    "import random\n",
    "import gc\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Jokes dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Cleaning raw reddit crawl file\n",
    "Removes unnecessary fields from the submissions and removes submissions that have been deleted or have no body."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Cleaning raw reddit crawl results - removes deleted submissions and removes unnecessary response values.\n",
    "def clean_crawl_results(source_fn, destination_fn):\n",
    "    crawled_df = pd.read_json(source_fn,orient='records')\n",
    "    \n",
    "    # Not all columns are needed for further analysis.\n",
    "    crawled_df.drop(columns=['created_utc', \"author\", \"subreddit\"], inplace=True)\n",
    "    \n",
    "    # Remove submissions, that have had their title or body removed.\n",
    "    crawled_df = crawled_df[~crawled_df['body'].isin([\"\",\"[removed]\",\"[deleted]\"])]\n",
    "    crawled_df = crawled_df[~crawled_df['title'].isin([\"\",\"[removed]\",\"[deleted]\"])]\n",
    "    \n",
    "    # Write json to an xz archive. xz seems to offer best compression ratio on these json files.\n",
    "    crawled_df.to_json(path_or_buf=destination_fn,orient='records',indent=4,compression=\"infer\")\n",
    "\n",
    "# src = \"data/raw_files/reddit_jokes_until_18-01-2021_uncleaned.json.xz\"\n",
    "# dst = \"data/cleaned_files/reddit_jokes.json.xz\"\n",
    "# clean_crawl_results(source_fn=src, destination_fn=dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Reddit dataset: cleaning\n",
    "Assumes that the initial cleaning from last block has been done.  \n",
    "The following blocks:\n",
    "- Remove anything from the post following \"edit: \";\n",
    "- Remove duplicate posts;\n",
    "- Create a \"joke\" column to the df by either combining titles with bodies or just returning bodies(if it contains the title);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning for jokes dataset\n",
    "def clean_df(df):\n",
    "    # Remove content after edit\n",
    "    df[\"title\"] = df[\"title\"].str.replace(r'edit:.*', '')\n",
    "    df[\"body\"] = df[\"body\"].str.replace(r'edit:.*', '')\n",
    "    \n",
    "    # Creating \"joke\" column\n",
    "    df[\"joke\"] = np.where(df[\"title\"].str[:10] != df[\"body\"].str[:10], df[\"title\"] + \" \" + df[\"body\"], df[\"body\"])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Read json, that has reddit submissions as \"title\" and \"body\", combine them into column \"joke\" and \n",
    "# remove duplicates.\n",
    "def read(json_filename):\n",
    "    df = pd.read_json(path_or_buf=json_filename,orient='records',compression=\"infer\")\n",
    "    df = clean_df(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading [[\"title\" and \"body\"]]\n",
    "# df = pd.read_json(path_or_buf=\"data/reddit_jokes.json.xz\",orient='records')\n",
    "\n",
    "try:\n",
    "    # Reading [[\"title\" and \"body\"]] and adding \"joke\" column\n",
    "    jokes_df = read(\"data/cleaned_files/reddit_jokes.json.xz\")\n",
    "    jokes_df[[\"text\", \"score\"]] = jokes_df[[\"joke\",\"score\"]]\n",
    "    \n",
    "    # Option to save all jokes (~1mil)\n",
    "    # jokes_df[[\"text\"]].to_json(path_or_buf=\"data/for_team/jokes_all.json.xz\")\n",
    "    \n",
    "    # Discart jokes with score < 1 (~200k discarded)\n",
    "    jokes_df = jokes_df[jokes_df[\"score\"]>0]\n",
    "    jokes_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    jokes_df.to_json(\"\")\n",
    "    jokes_df[[\"text\"]].to_json(path_or_buf=\"data/for_team/jokes.json.xz\")\n",
    "    \n",
    "    del jokes_df\n",
    "\n",
    "except Exception as e:\n",
    "    del jokes_df\n",
    "    gc.collect()\n",
    "    raise e\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Non-joke datasets\n",
    "Ideas:  \n",
    "https://lionbridge.ai/datasets/15-best-chatbot-datasets-for-machine-learning/  \n",
    "https://ai.google.com/research/NaturalQuestions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Google QA dataset: cleaning\n",
    "Cleaning the raw file was done in a separate script due to some file streaming errors in notebook.  \n",
    "Read json where each instance has {\"question\", \"long_answer\"} and combine them into one \"text\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started cutting.(tokenization)\n",
      "Finished tokenization.\n"
     ]
    }
   ],
   "source": [
    "# Join question/request and answer\n",
    "def create_text(qa):\n",
    "    q,a = qa\n",
    "    q = q.capitalize()\n",
    "    a = a.capitalize()\n",
    "    \n",
    "    # Questions/requests do not have the appropriate puncuations.\n",
    "    question_words = [\"How\",\"What\",\"When\",\"Where\",\"Which\",\"Who\",\"Whose\",\"Why\"]    \n",
    "    if any(q.startswith(qw) for qw in question_words):\n",
    "        q = q + \"?\"\n",
    "    else:\n",
    "        q = q + \".\"\n",
    "        \n",
    "    return q + \" \" + a\n",
    "\n",
    "# Questions with 1-10 sentences as answers seem to average out to same mean length as jokes.\n",
    "def shorten(arr):\n",
    "    end = random.randint(2,11)\n",
    "    new_arr = arr[0:end]\n",
    "    return new_arr\n",
    "\n",
    "\n",
    "try:\n",
    "    \n",
    "    qa_df = pd.read_json(\"data/cleaned_files/google_qa.json.xz\")\n",
    "    \n",
    "    # Join questions and answers into text\n",
    "    qa_df[[\"text\"]] = qa_df[[\"question\", \"long_answer\"]].apply(create_text, axis=1)\n",
    "    \n",
    "    qa_df = qa_df[[\"text\"]]\n",
    "\n",
    "    # Cut the articles to be about the same length of sentences as jokes.    \n",
    "    print(\"Started cutting.(tokenization)\")\n",
    "    qa_df[\"text\"] = qa_df[\"text\"].apply(str).apply(nltk.sent_tokenize).apply(shorten).apply(\" \".join)\n",
    "    print(\"Finished tokenization.\")\n",
    "\n",
    "    qa_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    print(\"Saving to file\")\n",
    "    qa_df.to_json(\"data/for_team/google_qa.json.xz\")\n",
    "    print(\"Finished saving to file\")\n",
    "    \n",
    "    del qa_df\n",
    "\n",
    "except Exception as e:\n",
    "    del qa_df\n",
    "    gc.collect()\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 News dataset: cleaning\n",
    "https://components.one/pages/about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Siim Markus Marvet\\Miniconda3\\envs\\ICNS_tensor\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3147: DtypeWarning: Columns (1,3,5) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading.\n",
      "Removed publication names.\n",
      "Started tokenization.\n",
      "Finished tokenization.\n",
      "Writing to file\n",
      "Finished processing.\n"
     ]
    }
   ],
   "source": [
    "# Articles with lengths of 1-10 sentences seem to average out to 4 sentences per article\n",
    "# Note that sentence length std will still differ from jokes.\n",
    "def shorten(arr):\n",
    "    end = random.randint(2,11)\n",
    "    new_arr = arr[1:end]\n",
    "    return new_arr\n",
    "\n",
    "try:\n",
    "    # Read in all news\n",
    "    news_df = pd.read_csv(\"data/all-the-news-2-1.zip\")\n",
    "    print(\"Finished reading.\")\n",
    "    # Print count of each publications articles\n",
    "    # news_df.groupby(by='publication').count()[\"article\"]\n",
    "\n",
    "    # Keep only important columns\n",
    "    news_df=news_df[[\"article\", \"publication\"]]\n",
    "\n",
    "    # Keep only articles from certain publishers\n",
    "    #publications = ['New Republic', 'TMZ', 'Business Insider', 'Vox', 'CNBC', 'People', 'TechCrunch', 'Refinery 29', 'The Hill', 'Wired', 'Vice News', 'Economist', 'New Yorker', 'Washington Post', 'The New York Times', 'Buzzfeed News', 'Reuters', 'The Verge', 'Fox News', 'Hyperallergic', 'Gizmodo', 'Axios', 'CNN', 'Politico', 'Mashable', 'Vice']\n",
    "    publications = [\"Washington Post\", \"The New York Times\", \"Reuters\"]\n",
    "    news_df=news_df[news_df[\"publication\"].str.contains('|'.join(publications),na = False)]\n",
    "    \n",
    "    # Drop empty rows\n",
    "    news_df = news_df[news_df[\"article\"] != \"\"]\n",
    "    # Drop NA\n",
    "    news_df.dropna(inplace=False)\n",
    "    \n",
    "    # Shuffle df\n",
    "    news_df = news_df.sample(frac=1, random_state=0)\n",
    "    \n",
    "    # Select first 500,000 articles \n",
    "    #(some articles will end up with no content after processing, don't have time to figure out why, \n",
    "    # so will just include a margin to have at least 300,000 articles with content by the end).\n",
    "    news_df = news_df.head(500000)\n",
    "    \n",
    "    # Remove publication names from articles\n",
    "    \n",
    "    news_df[\"article\"] = news_df[\"article\"].str.replace(\"|\".join(publications), '', case = False, regex=True)\n",
    "    print(\"Removed publication names.\")\n",
    "\n",
    "    # Cut the articles to be about the same length of sentences as jokes.\n",
    "    print(\"Started cutting sentences.(tokenization)\")\n",
    "    news_df[\"article\"] = news_df[\"article\"].apply(str).apply(nltk.sent_tokenize).apply(shorten).apply(\" \".join)\n",
    "    print(\"Finished tokenization.\")\n",
    "    \n",
    "    # Reset id column\n",
    "    news_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Leave only article column\n",
    "    news_df=news_df[[\"text\"]]\n",
    "    \n",
    "    # Write to file\n",
    "    print(\"Writing to file\")\n",
    "    news_df.to_json(path_or_buf=\"data/for_team/articles.json.xz\",orient='records',indent=4,compression=\"infer\")\n",
    "    print(\"Finished processing.\")\n",
    "    \n",
    "    del news_df\n",
    "\n",
    "except Exception as e:\n",
    "    del news_df\n",
    "    gc.collect()\n",
    "    raise e\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Creating tokenized files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(df, column_name):\n",
    "    df[\"word_tokenize\"] = df[column_name].apply(nltk.word_tokenize)\n",
    "    print(\"Finished word tokenization.\")\n",
    "    \n",
    "    df[\"sent_tokenize\"] = df[column_name].apply(nltk.sent_tokenize)\n",
    "    print(\"Finished sentence tokenization.\")\n",
    "    \n",
    "    return df\n",
    "    \n",
    "def print_tokenization_stats(df):\n",
    "    print(f'Words mean: {df[\"word_tokenize\"].apply(len).mean():.2f}')\n",
    "    print(f'Words STD: {df[\"word_tokenize\"].apply(len).std():.2f}')\n",
    "    \n",
    "    print(f'Sentences mean: {df[\"sent_tokenize\"].apply(len).mean()}')\n",
    "    print(f'Sentences STD: {df[\"sent_tokenize\"].apply(len).std():.2f}')\n",
    "\n",
    "def create_tokenized_file(source_fn, destination_fn):\n",
    "    df = pd.read_json(source_fn)\n",
    "    df = tokenize(df, \"text\")\n",
    "    print_tokenization_stats(df)\n",
    "    print(f'Writing to file')\n",
    "    df.to_json(destination_fn,orient='records',compression=\"infer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished word tokenization.\n",
      "Finished sentence tokenization.\n",
      "Words mean: 55.71\n",
      "Words STD: 117.59\n",
      "Sentences mean: 3.8667775796982577\n",
      "Sentences STD: 7.75\n"
     ]
    }
   ],
   "source": [
    "src = \"data/for_team/jokes.json.xz\"\n",
    "dest = \"data/stats_files/tokenized_jokes.json.xz\"\n",
    "create_tokenized_file(source_fn=src, destination_fn=dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### News articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished word tokenization.\n",
      "Finished sentence tokenization.\n",
      "Words mean: 140.06\n",
      "Words STD: 92.44\n",
      "Sentences mean: 4.842046666666667\n",
      "Sentences STD: 2.79\n"
     ]
    }
   ],
   "source": [
    "src = \"data/for_team/articles.json.xz\"\n",
    "dest = \"data/stats_files/tokenized_news.json.xz\"\n",
    "create_tokenized_file(source_fn=src, destination_fn=dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished word tokenization.\n",
      "Finished sentence tokenization.\n",
      "Words mean: 127.61\n",
      "Words STD: 394.49\n",
      "Sentences mean: 3.8504325363646124\n",
      "Sentences STD: 1.92\n"
     ]
    }
   ],
   "source": [
    "src = \"data/for_team/google_qa.json.xz\"\n",
    "dest = \"data/stats_files/tokenized_google_qa.json.xz\"\n",
    "create_tokenized_file(source_fn=src, destination_fn=dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Adding lemmatization to tokenized files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lemmatizer:\n",
    "    def __init__(self):\n",
    "        self.internal_lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    def lemmatize_arr(self, tokens):\n",
    "        lemmas = [self.internal_lemmatizer.lemmatize(t.lower()) for t in tokens]\n",
    "        return lemmas\n",
    "\n",
    "def create_lemmatized_file(source_fn, destination_fn):\n",
    "    lemmatizer = Lemmatizer()\n",
    "    df = pd.read_json(source_fn)\n",
    "    \n",
    "    df[\"lemmatize\"] = df[\"word_tokenize\"].apply(lemmatizer.lemmatize_arr)\n",
    "    print(f'Finished lemmatizing file \"{source_fn}\"')\n",
    "    \n",
    "    df.to_json(destination_fn,orient='records',compression=\"infer\")\n",
    "    print(f'Finished writing to file \"{destination_fn}\"')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished lemmatizing file \"data/stats_files/tokenized_jokes.json.xz\"\n",
      "Finished writing to file \"data/stats_files/tokenized_lemmatized_jokes.json.xz\"\n"
     ]
    }
   ],
   "source": [
    "src = f'data/stats_files/tokenized_jokes.json.xz'\n",
    "dest = f'data/stats_files/tokenized_lemmatized_jokes.json.xz'\n",
    "create_lemmatized_file(source_fn=src, destination_fn=dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = f'data/stats_files/tokenized_news.json.xz'\n",
    "dest = f'data/stats_files/tokenized_lemmatized_news.json.xz'\n",
    "create_lemmatized_file(source_fn=src, destination_fn=dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished lemmatizing file \"data/stats_files/tokenized_google_qa.json.xz\"\n",
      "Finished writing to file \"data/stats_files/tokenized_lemmatized_google_qa.json.xz\"\n"
     ]
    }
   ],
   "source": [
    "src = f'data/stats_files/tokenized_google_qa.json.xz'\n",
    "dest = f'data/stats_files/tokenized_lemmatized_google_qa.json.xz'\n",
    "create_lemmatized_file(source_fn=src, destination_fn=dest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
