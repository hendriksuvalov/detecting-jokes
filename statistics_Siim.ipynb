{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the statistics of jokes and none-jokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data storage\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Language processing\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#Other\n",
    "import random\n",
    "import gc\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General stats about the count of words and sentences\n",
    "def print_tokenization_stats(df):\n",
    "    print(f'Text instances: {df.shape[0]:,}\\n')\n",
    "    \n",
    "    word_lengths_series = df[\"word_tokenize\"].apply(len)\n",
    "    print(f'Words total: {word_lengths_series.sum():,}')\n",
    "    print(f'Words mean: {word_lengths_series.mean():.2f}')\n",
    "    print(f'Words std: {word_lengths_series.std():.2f}')\n",
    "    print()\n",
    "      \n",
    "    sentence_lengths_series = df[\"sent_tokenize\"].apply(len)\n",
    "    print(f'Sentences total: {sentence_lengths_series.sum():,}')\n",
    "    print(f'Sentences mean: {sentence_lengths_series.mean():.2f}')\n",
    "    print(f'Sentences std: {sentence_lengths_series.std():.2f}')\n",
    "    print()\n",
    "    \n",
    "\n",
    "def repeating_words_and_lemmas(df):\n",
    "    def unique_proportion(arr):\n",
    "        new_arr = [x for x in arr if x not in string.punctuation]\n",
    "        \n",
    "        if len(new_arr) == 0:\n",
    "            return 0\n",
    "        \n",
    "        return len(set(new_arr))/len(new_arr)\n",
    "    \n",
    "    unique_words_proportion = df[\"word_tokenize\"].apply(unique_proportion).mean()\n",
    "    print(f'Mean proportion of unique words in text instance: {unique_words_proportion:.4f}.')\n",
    "\n",
    "    unique_lemmas_proportion = df[\"lemmatize\"].apply(unique_proportion).mean()    \n",
    "    print(f'Mean proportion of unique lemmas in text instance: {unique_lemmas_proportion:.4f}.')\n",
    "    print()\n",
    "    \n",
    "\n",
    "# Creates and returns FreqDist's for use in later cells\n",
    "def word__frequency_analysis(df, lower=False, lemmatize=False):\n",
    "    freq_dist = FreqDist()\n",
    "    unique_freq_dist = FreqDist()\n",
    "\n",
    "    if lemmatize:\n",
    "        token_column = \"lemmatize\"\n",
    "    else:\n",
    "        token_column = \"word_tokenize\"\n",
    "\n",
    "    for tokens in df[token_column]:\n",
    "        if lower:\n",
    "            tokens = [t.lower() for t in tokens]\n",
    "\n",
    "        freq_dist.update(tokens)\n",
    "        unique_freq_dist.update(set(tokens))\n",
    "            \n",
    "    \n",
    "        \n",
    "    print(\"General frequencies:\")\n",
    "    freq_dist.pprint(20)\n",
    "    print(\"\\nUnique frequencies(each token counted only once per text instance):\")\n",
    "    unique_freq_dist.pprint(20)\n",
    "    print()\n",
    "    \n",
    "    return freq_dist, unique_freq_dist\n",
    "\n",
    "\n",
    "# What part of text instances contain certain most common words\n",
    "def word_coverage_analysis(df, freq_dist, unique_freq_dist):\n",
    "    for n_most_common in [10,50,100]:\n",
    "        freq_sum = sum(unique_freq_dist.freq(token[0]) for token in unique_freq_dist.most_common(n_most_common))\n",
    "        print(f'Top {n_most_common} most frequent words make up {freq_sum*100:.2f}% of all words.')\n",
    "    \n",
    "    print()\n",
    "    \n",
    "# Combines the functions above\n",
    "def analyze_file(filename):\n",
    "    df = pd.read_json(filename)\n",
    "    \n",
    "    print_tokenization_stats(df)\n",
    "    \n",
    "    repeating_words_and_lemmas(df)\n",
    "    \n",
    "    freq_dist, unique_freq_dist = word__frequency_analysis(df, lemmatize=True)\n",
    "    \n",
    "    word_coverage_analysis(df, freq_dist, unique_freq_dist)\n",
    "    \n",
    "    return freq_dist, unique_freq_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data from tokenized files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jokes\n",
      "Text instances: 772,978\n",
      "\n",
      "Words total: 43,063,137\n",
      "Words mean: 55.71\n",
      "Words std: 117.59\n",
      "\n",
      "Sentences total: 2,988,934\n",
      "Sentences mean: 3.87\n",
      "Sentences std: 7.75\n",
      "\n",
      "Mean proportion of unique words in text instance: 0.8591.\n",
      "Mean proportion of unique lemmas in text instance: 0.8380.\n",
      "\n",
      "General frequencies:\n",
      "FreqDist({'the': 2032115, '.': 1861586, ',': 1710063, 'a': 1368527, 'and': 945609, 'to': 862682, 'i': 817273, \"''\": 654350, '?': 597624, '``': 596776, 'you': 591393, 'he': 507199, 'of': 441433, 'it': 432912, 'in': 430020, 'is': 335333, '!': 324761, 'his': 305537, 'that': 304498, 'what': 298114, ...})\n",
      "\n",
      "Unique frequencies(each token counted only once per text instance):\n",
      "FreqDist({'.': 522758, 'a': 473859, 'the': 443916, '?': 412893, 'i': 325779, 'to': 319681, ',': 316927, 'and': 287260, 'you': 284807, 'what': 239446, 'it': 237791, 'in': 228834, 'of': 221014, 'is': 190367, 'do': 187011, 'my': 177444, \"'s\": 177319, 'he': 174033, 'that': 165475, \"''\": 164576, ...})\n",
      "\n",
      "Top 10 most frequent words make up 14.22% of all words.\n",
      "Top 50 most frequent words make up 35.10% of all words.\n",
      "Top 100 most frequent words make up 46.67% of all words.\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "news\n",
      "Text instances: 300,000\n",
      "\n",
      "Words total: 42,018,000\n",
      "Words mean: 140.06\n",
      "Words std: 92.44\n",
      "\n",
      "Sentences total: 1,452,614\n",
      "Sentences mean: 4.84\n",
      "Sentences std: 2.79\n",
      "\n",
      "Mean proportion of unique words in text instance: 0.7629.\n",
      "Mean proportion of unique lemmas in text instance: 0.7381.\n",
      "\n",
      "General frequencies:\n",
      "FreqDist({'the': 2174373, ',': 1964525, '.': 1375651, 'a': 1114001, 'to': 970662, 'of': 926024, 'in': 877991, 'and': 849588, '’': 510022, 's': 409933, 'on': 389900, 'it': 380580, 'for': 352124, 'that': 337874, '“': 325767, 'by': 316826, '”': 315801, 'said': 308402, 'is': 253654, 'with': 245549, ...})\n",
      "\n",
      "Unique frequencies(each token counted only once per text instance):\n",
      "FreqDist({'.': 293478, 'the': 282768, ',': 279207, 'a': 257906, 'to': 250257, 'in': 249982, 'of': 247984, 'and': 244266, '’': 199828, 'on': 187888, 's': 184100, 'for': 178212, 'it': 168358, 'by': 164814, 'that': 162416, 'said': 160897, '“': 151205, '”': 149301, 'with': 144058, 'is': 139370, ...})\n",
      "\n",
      "Top 10 most frequent words make up 9.38% of all words.\n",
      "Top 50 most frequent words make up 24.57% of all words.\n",
      "Top 100 most frequent words make up 32.23% of all words.\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "google_qa\n",
      "Text instances: 307,373\n",
      "\n",
      "Words total: 39,224,167\n",
      "Words mean: 127.61\n",
      "Words std: 394.49\n",
      "\n",
      "Sentences total: 1,183,519\n",
      "Sentences mean: 3.85\n",
      "Sentences std: 1.92\n",
      "\n",
      "Mean proportion of unique words in text instance: 0.7231.\n",
      "Mean proportion of unique lemmas in text instance: 0.7023.\n",
      "\n",
      "General frequencies:\n",
      "FreqDist({'the': 1718227, ',': 1628105, '.': 903564, 'of': 822095, '(': 717856, ')': 716900, 'a': 668280, 'in': 628283, 'and': 609112, '-': 522993, '``': 456661, 'to': 422876, '--': 378175, 'is': 298999, '?': 233734, '/': 228032, 'by': 203128, 'on': 196633, ':': 193067, 'wa': 185054, ...})\n",
      "\n",
      "Unique frequencies(each token counted only once per text instance):\n",
      "FreqDist({'the': 284067, '.': 275409, ',': 250511, 'of': 240205, 'in': 230520, '?': 228658, 'and': 216185, 'a': 214287, 'to': 172468, 'is': 157874, '(': 138930, ')': 138533, 'by': 117964, 'on': 114481, '-': 112920, 'for': 108675, 'wa': 104458, 'who': 99525, \"'s\": 95719, 'it': 92665, ...})\n",
      "\n",
      "Top 10 most frequent words make up 10.89% of all words.\n",
      "Top 50 most frequent words make up 23.57% of all words.\n",
      "Top 100 most frequent words make up 29.36% of all words.\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Wall time: 3min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "files = [\"jokes\", \"news\", \"google_qa\"]\n",
    "freq_dists = {}\n",
    "unique_freq_dists = {}\n",
    "\n",
    "for f in files:\n",
    "    print(f)\n",
    "    freq_dist, unique_freq_dist = analyze_file(f'data/stats_files/tokenized_lemmatized_{f}.json.xz')\n",
    "    freq_dists[f] = freq_dist\n",
    "    unique_freq_dists[f] = unique_freq_dist\n",
    "    print(\"\\n--------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differences in most frequent words\n",
    "Unique nltk.word_tokenize() tokens among the {nr} most common tokens in each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens that appear among the 200 most frequent words in dataset, but not among the top 200 of other 2 datasets.\n",
      "\n",
      "In joke but not in others: \n",
      "{'look', 'once', 'another', 'replied', 'am', 'asked', 'very', 'see', 'ever', 'again', 'every', 'go', 'man', 'friend', 'me', 'get', 'give', 'thing', 'guy', 'bar', 'put', 'why', 'few', 'wife', 'sex', 'think', 'know', 'my', 'hear', 'old', 'girl', \"'re\", 'came', 'head', 'need', 'sure', '*', 'thought', 'joke', 'let', 'find', 'tell', \"'ve\", 'going', '!', 'call', 'night', 'little', 'turn', 'should', '...', 'too', \"'ll\", 'went', \"n't\", 'today', 'walk', 'difference', 'really', 'oh', 'start', 'always', \"'m\", 'woman', 'long', 'ca', 'good', 'want', 'your', 'hand', 'reply', 'got', 'asks', 'never'}\n",
      "\n",
      "In news but not in others: \n",
      "{'against', 'global', 'statement', 'week', 'according', 'billion', '%', 'public', 'economy', 'tuesday', '—', 'government', 'share', 'set', 'trump', 'expected', 'month', 'reporting', 'editing', 'source', 'news', 'friday', 'price', 'official', 'wednesday', 'china', 'bank', 'monday', 'our', 'european', 'point', 'four', 'executive', 'chief', 'report', 'trade', 'added', 'mr.', 'investor', 'financial', 'minister', 'u.s.', 'former', 'million', 'company', 'president', 'those', 'thursday', 'case', 'group', '$', 'earlier', 'market', 'percent', 'business', 'comment', 'deal', 'plan'}\n",
      "\n",
      "In qa but not in others: \n",
      "{'team', '15', 'september', 'title', 'released', 'each', '2013', 'north', 'song', '22', 'played', '13', 'system', 'based', '4', 'role', 'u', '30', 'march', 'movie', '9', '8', 'july', 'april', 'january', 'these', '17', 'english', 'play', 'season', '11', '24', 'india', 'single', '6', '12', '&', 'original', '5', 'south', 'used', 'episode', 'written', 'won', 'character', '25', 'known', 'august', 'called', 'number', 'date', '3', 'name', 'series', 'film', 'show', '--', '2016', '2015', 'war', '2', '20', 'place', '18', 'best', '14', '2017', '21', 'music', '16', 'june', '\\\\', 'area', '7', '19', 'october', '/', '2014', 'john', '2018', 'following', 'game'}\n"
     ]
    }
   ],
   "source": [
    "# most_common_count - Number of most common words taken from each dataset\n",
    "def token_differences(freq_dists, unique_freq_dists, most_common_count = 100):\n",
    "    most_common_uniques = {}\n",
    "    \n",
    "    for key in unique_freq_dists.keys():\n",
    "        uniques = set(x[0] for x in unique_freq_dists[key].most_common(most_common_count))\n",
    "        most_common_uniques[key] = uniques\n",
    "        \n",
    "        \n",
    "    print(f'In joke but not in others: \\n{most_common_uniques[\"jokes\"] - most_common_uniques[\"news\"] - most_common_uniques[\"google_qa\"]}')\n",
    "    print()\n",
    "    print(f'In news but not in others: \\n{most_common_uniques[\"news\"] - most_common_uniques[\"jokes\"] - most_common_uniques[\"google_qa\"]}')\n",
    "    print()\n",
    "    print(f'In qa but not in others: \\n{most_common_uniques[\"google_qa\"] - most_common_uniques[\"jokes\"] - most_common_uniques[\"news\"]}')\n",
    "\n",
    "# This number can be changed\n",
    "most_common_count = 200\n",
    "print(f'Tokens that appear among the {most_common_count} most frequent words in dataset, but not among the top {most_common_count} of other 2 datasets.\\n')\n",
    "token_differences(freq_dists, unique_freq_dists, most_common_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now between jokes and non-jokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens that appear among the 200 most frequent words in dataset, but not among the top 200 of other 2 datasets.\n",
      "\n",
      "In joke: \n",
      "{'look', 'again', 'every', 'right', 'me', 'give', 'guy', 'three', 'bar', 'put', 'few', 'sex', 'think', 'hear', 'old', 'just', 'girl', 'well', 'sure', 'she', 'find', 'tell', \"'ve\", 'going', 'should', '...', 'too', 'any', 'today', 'asks', 'another', 'friend', 't', 'thing', 'still', 'know', 'like', 'need', 'life', 'thought', 'little', 'turn', \"'ll\", 'now', 'second', 'walk', 'ca', 'good', 'want', 'hand', 'so', 'make', 'replied', 'very', 'see', 'go', 'man', 'wife', 'later', 'my', 'her', 'here', 'then', '*', 'much', 'joke', 'next', '!', 'because', 'call', 'night', 'went', \"n't\", 'difference', 'really', 'oh', 'start', 'way', 'around', 'woman', 'long', 'take', 'say', 'got', 'before', 'once', 'am', 'even', 'asked', 'ever', 'get', 'you', 'why', 'off', \"''\", 'down', \"'re\", 'came', 'head', 'let', 'back', 'home', 'him', 'them', 'always', \"'m\", 'your', 'reply', 'work', 'never'}\n",
      "\n",
      "In non joke: \n",
      "{'is', '.', 'out', 'we', 'week', 'but', 'not', 'or', 'billion', 'at', 'there', 'on', 'be', 'known', '“', 'world', 'up', 'and', 'ha', '—', '3', 'name', 'state', 'may', 'editing', '-', 'have', 'after', 'when', 'that', 'song', 'more', 'i', 'series', 'film', 'first', '--', 'for', 'two', 'he', 'it', 'people', 'while', 'which', 'to', 'the', '4', '1', 'during', 'than', 'of', 'would', 'since', 'could', 'million', '2', 'can', 'a', 'who', 'new', 'said', 'group', 'market', 'were', 'united', 'over', 'been', '(', '2017', 'into', 'by', 'statement', 'day', 'what', 'one', 'last', 'about', 'from', 'if', 'this', 'season', 'year', 'government', 'his', 'all', 'time', 'month', 'in', 'reporting', ')', ':', 'also', ',', '7', '6', 'country', 'other', 'are', 'told', '/', 'their', 'an', 'most', 's', 'with', 'had', 'u.s.', '”', 'company', '5', 'president', '10', ';', 'will', 'no', 'american', '$', 'some', 'used', '’', 'percent', 'they', 'wa'}\n"
     ]
    }
   ],
   "source": [
    "# most_common_count - Number of most common words taken from each dataset\n",
    "def token_differences(freq_dists, unique_freq_dists, most_common_count = 100):\n",
    "    most_common_uniques = {}\n",
    "    \n",
    "    \n",
    "    most_common_uniques[\"jokes\"] = set(x[0] for x in unique_freq_dists[\"jokes\"].most_common(most_common_count))\n",
    "    most_common_uniques[\"google_qa\"] = set(x[0] for x in unique_freq_dists[\"google_qa\"].most_common(int(most_common_count/2)))\n",
    "    most_common_uniques[\"news\"] = set(x[0] for x in unique_freq_dists[\"news\"].most_common(int(most_common_count/2)))\n",
    "\n",
    "        \n",
    "        \n",
    "    print(f'In joke: \\n{most_common_uniques[\"jokes\"] - most_common_uniques[\"news\"] - most_common_uniques[\"google_qa\"]}')\n",
    "    print()\n",
    "    print(f'In non joke: \\n{most_common_uniques[\"news\"] | most_common_uniques[\"google_qa\"] - most_common_uniques[\"jokes\"]}')\n",
    "    \n",
    "\n",
    "\n",
    "# This number can be changed\n",
    "most_common_count = 200\n",
    "print(f'Tokens that appear among the {most_common_count} most frequent words in dataset, but not among the top {most_common_count} of other 2 datasets.\\n')\n",
    "token_differences(freq_dists, unique_freq_dists, most_common_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset specific statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jokes analysis\n",
    "Requires the raw jokes file(\"reddit_jokes.json.xz\"), that has separate columns for title, body and joke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can be used for printing some metrics for jokes\n",
    "def jokes_stats(df):\n",
    "    # Counts of rows, that match certain conditions\n",
    "    df_len = len(df)\n",
    "    edited = len(df[df['title'].str.contains(\"edit\") | df['body'].str.contains(\"edit\")])\n",
    "    body_contains_title = len(df[df[\"title\"].str[:10] == df[\"body\"].str[:10]])\n",
    "    unique_body = len(df[\"body\"].unique())\n",
    "    unique_title = len(df[\"title\"].unique())\n",
    "    unique_joke = len(df[\"joke\"].unique())\n",
    "    \n",
    "    \n",
    "    print(f'Total jokes: {df_len:,}')\n",
    "    print(f'Title in body: {body_contains_title:,} ({body_contains_title/df_len*100:.2f}%)')\n",
    "    print(f'Edited: {edited:,} ({edited/df_len*100:.2f}%)')\n",
    "    print(f'Unique body: {unique_body:,} ({unique_body/df_len*100:.2f})%')\n",
    "    print(f'Unique title: {unique_title:,} ({unique_title/df_len*100:.2f}%)')\n",
    "    print(f'Unique joke(title+body): {unique_joke:,} ({unique_joke/df_len*100:.2f}%)')\n",
    "    \n",
    "    print()\n",
    "    print(f'Avg chars per entry: {df[\"joke\"].apply(len).mean()}')\n",
    "\n",
    "# Cant find the correct file, might have deleted id. In any case, this was mostly important in cleaning jokes.\n",
    "# jokes_stats(pd.read_json(\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def print_examples(file, column):\n",
    "    df = pd.read_json(f'data/stats_files/tokenized_lemmatized_{file}.json.xz')\n",
    "    i = 0\n",
    "    for text in df[\"text\"]:\n",
    "        print(text)\n",
    "        print(\"\\n----------------------------------------------------------\\n\")\n",
    "        if i > 50:\n",
    "            break\n",
    "        i+=1\n",
    "\n",
    "#print_examples(file=\"news\", column=\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp():\n",
    "    df = pd.read_json(\"data/for_team/jokes.json.xz\")\n",
    "    print(df[[\"text\"]])\n",
    "    \n",
    "temp()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
